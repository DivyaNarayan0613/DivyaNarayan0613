{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOiBUdr202O8TAGxexpEJcy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DivyaNarayan0613/DivyaNarayan0613/blob/main/GENAI_VAEs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch â€“ The core PyTorch library for tensor computations, similar to NumPy but optimized for GPU acceleration.\n",
        "\n",
        "torch.nn â€“ Provides modules and functions for building neural networks, including layers, activation functions, and loss functions.\n",
        "\n",
        "torch.optim â€“ Contains optimization algorithms (e.g., SGD, Adam) used for training neural networks by adjusting model weights.\n",
        "\n",
        "torchvision.datasets â€“ Provides access to popular datasets (like MNIST, CIFAR-10) and utilities for loading image data.\n",
        "\n",
        "torchvision.transforms â€“ Offers data transformation utilities (e.g., normalization, resizing, augmentation) for preprocessing images.\n",
        "\n",
        "torch.utils.data.DataLoader â€“ Facilitates efficient loading and batching of datasets, enabling parallel processing.\n",
        "\n",
        "matplotlib.pyplot â€“ Used for visualizing data, such as plotting training loss curves or displaying images from the dataset.\n",
        "\n",
        "numpy â€“ Provides numerical computing capabilities and array operations, often used for preprocessing and analysis."
      ],
      "metadata": {
        "id": "Tb7ePtOvndOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CHvDeStspSR"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.cuda.is_available(): Checks if a compatible NVIDIA GPU is available and if PyTorch has CUDA support enabled.\n",
        "If True â†’ Uses \"cuda\" (GPU) for faster computations.\n",
        "If False â†’ Falls back to \"cpu\" (slower but works universally).\n",
        "\n",
        "GPU acceleration: Deep learning models train much faster on GPUs due to parallel processing.\n",
        "\n",
        "Portability: This approach makes your code flexibleâ€”it will automatically use a GPU if available, otherwise, it runs on the CPU."
      ],
      "metadata": {
        "id": "RYfXBwKUngQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check device (GPU recommended)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSl4zcicstEp",
        "outputId": "c5703c22-50a0-4ae5-d456-6fb932e0246a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "latent_dim = 100\n",
        "\n",
        "    This represents the dimension of the noise vector (z), typically used in GANs.\n",
        "    The generator takes this as input to produce realistic-looking images.\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "    Determines how many samples are processed before updating the model weights.\n",
        "    A larger batch size can stabilize training but requires more memory.\n",
        "\n",
        "lr = 0.0002 (Learning Rate)\n",
        "\n",
        "    Controls how much the modelâ€™s weights change at each step.\n",
        "    A value of 0.0002 is commonly used in GANs (e.g., DCGAN).\n",
        "    Too high â†’ Model might not converge.\n",
        "    Too low â†’ Training might be too slow.\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "    The number of times the entire dataset is passed through the model.\n",
        "    More epochs allow the model to learn better, but too many can lead to overfitting."
      ],
      "metadata": {
        "id": "ifP62M2vn9eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "latent_dim = 100    # Dimension of the noise vector\n",
        "batch_size = 128    # Batch size\n",
        "lr = 0.0002         # Learning rate\n",
        "epochs = 50         # Number of epochs\n"
      ],
      "metadata": {
        "id": "3SG1bEvlsvNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transforms.ToTensor() â†’ Converts the image to a PyTorch tensor (shape: [C, H, W], values in [0,1]).\n",
        "transforms.Normalize([0.5], [0.5]) â†’ Normalizes pixel values to [-1, 1].\n",
        "\n",
        "    Original pixel values: [0, 255]\n",
        "    After .ToTensor(): [0, 1]\n",
        "    Normalization formula: (x - mean) / std â†’ (x - 0.5) / 0.5 â†’ results in values between -1 and 1\n",
        "    This is often used in GANs (e.g., DCGAN) since Tanh activation performs better with normalized inputs"
      ],
      "metadata": {
        "id": "qvZZun67oHtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "datasets.MNIST â†’ Loads the MNIST dataset (handwritten digit images, 28x28, grayscale).\n",
        "\n",
        "root=\"./data\" â†’ Saves the dataset in the ./data directory.\n",
        "\n",
        "train=True â†’ Loads the training set (if False, loads test set).\n",
        "\n",
        "transform=transform â†’ Applies the defined transformations (ToTensor and Normalize).\n",
        "\n",
        "download=True â†’ Downloads the dataset if not already available."
      ],
      "metadata": {
        "id": "NfUDhEFOoJ6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader â†’ Efficiently loads data in batches for training.\n",
        "\n",
        "batch_size=batch_size â†’ Loads 128 samples per batch (as set earlier).\n",
        "\n",
        "shuffle=True â†’ Shuffles the dataset each epoch to improve training performance."
      ],
      "metadata": {
        "id": "gcAxnQtGoQc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
        "])\n",
        "mnist_data = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "data_loader = DataLoader(mnist_data, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVMhJjDgsxk4",
        "outputId": "29eb5b31-5ed8-4995-9113-925d2fc94de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 16.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 491kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 4.44MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 4.15MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A VAE is a type of neural network that combines ideas from:\n",
        "\n",
        "    Autoencoders: Used for learning a compressed representation (encoding) of data.\n",
        "    Probabilistic Modeling: Learns to approximate the data distribution and generate new samples.\n",
        "\n",
        "It has two main components:\n",
        "\n",
        "    Encoder: Encodes input into a latent space (a distribution).\n",
        "    Decoder: Reconstructs the input from the latent space.\n",
        "\n",
        "The VAE class inherits from nn.Module and has three main components:\n",
        "\n",
        "    Encoder\n",
        "    Latent Space\n",
        "    Decoder\n",
        "\n"
      ],
      "metadata": {
        "id": "5B-SoGP-GTpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder**\n",
        "\n",
        "self.encoder = nn.Sequential(\n",
        "    nn.Linear(28 * 28, 400),\n",
        "    nn.ReLU()\n",
        ")\n",
        "\n",
        "self.fc_mu = nn.Linear(400, latent_dim)  # Mean of latent distribution\n",
        "\n",
        "self.fc_logvar = nn.Linear(400, latent_dim)  # Log-variance of latent\n",
        "distribution\n",
        "\n",
        "The Encoder reduces the input dimension (flattened 28x28 image) to a hidden representation (h of size 400).\n",
        "\n",
        "Two separate linear layers (fc_mu and fc_logvar) compute:\n",
        "\n",
        "    mu: The mean of the latent distribution.\n",
        "    logvar: The logarithm of the variance of the latent distribution.\n",
        "\n",
        "This latent distribution is parameterized as a Gaussian distribution."
      ],
      "metadata": {
        "id": "M2dl9sfUG2nF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reparameterization Trick**\n",
        "\n",
        "def reparameterize(self, mu, logvar):\n",
        "\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "\n",
        "    eps = torch.randn_like(std)\n",
        "\n",
        "    return mu + eps * std\n",
        "\n",
        "Instead of sampling directly from the latent distribution (which isn't differentiable), the reparameterization trick is used:\n",
        "\n",
        "    Compute std (standard deviation) from logvar.\n",
        "    Generate random noise eps from a standard normal distribution.\n",
        "    Combine them\n"
      ],
      "metadata": {
        "id": "kZyMXYF8HUYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder reconstructs the input from the latent variable z."
      ],
      "metadata": {
        "id": "8Sl08JwQHuxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function balances reconstruction quality and regularization of the latent space."
      ],
      "metadata": {
        "id": "uC2VU2JGHv8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the VAE architecture\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 400),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(400, latent_dim)  # Mean of latent distribution\n",
        "        self.fc_logvar = nn.Linear(400, latent_dim)  # Log-variance of latent distribution\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 28 * 28),\n",
        "            nn.Tanh()  # Output normalized to [-1, 1]\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 28 * 28))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        reconstructed = self.decode(z)\n",
        "        return reconstructed, mu, logvar\n",
        "\n",
        "# Loss function for VAE\n",
        "def vae_loss(reconstructed, original, mu, logvar):\n",
        "    # Reconstruction loss (Binary Cross-Entropy)\n",
        "    bce_loss = nn.functional.binary_cross_entropy_with_logits(reconstructed, original.view(-1, 28 * 28), reduction='sum')\n",
        "    # KL divergence\n",
        "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return bce_loss + kl_divergence\n",
        "\n"
      ],
      "metadata": {
        "id": "T8UFhFlasz6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, optimizer\n",
        "vae = VAE().to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=lr)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for imgs, _ in data_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        reconstructed, mu, logvar = vae(imgs)\n",
        "        loss = vae_loss(reconstructed, imgs, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {train_loss / len(data_loader.dataset):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHuAz0xt8Mi3",
        "outputId": "6a0a40dd-a371-485b-d361-6b25db65cb90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: -284.8522\n",
            "Epoch [2/50], Loss: -327.8985\n",
            "Epoch [3/50], Loss: -329.5570\n",
            "Epoch [4/50], Loss: -330.6255\n",
            "Epoch [5/50], Loss: -331.5967\n",
            "Epoch [6/50], Loss: -332.6011\n",
            "Epoch [7/50], Loss: -333.2634\n",
            "Epoch [8/50], Loss: -333.4425\n",
            "Epoch [9/50], Loss: -333.4845\n",
            "Epoch [10/50], Loss: -333.5014\n",
            "Epoch [11/50], Loss: -333.5097\n",
            "Epoch [12/50], Loss: -333.5149\n",
            "Epoch [13/50], Loss: -333.5181\n",
            "Epoch [14/50], Loss: -333.5203\n",
            "Epoch [15/50], Loss: -333.5220\n",
            "Epoch [16/50], Loss: -333.5232\n",
            "Epoch [17/50], Loss: -333.5241\n",
            "Epoch [18/50], Loss: -333.5248\n",
            "Epoch [19/50], Loss: -333.5253\n",
            "Epoch [20/50], Loss: -333.5257\n",
            "Epoch [21/50], Loss: -333.5261\n",
            "Epoch [22/50], Loss: -333.5263\n",
            "Epoch [23/50], Loss: -333.5265\n",
            "Epoch [24/50], Loss: -333.5267\n",
            "Epoch [25/50], Loss: -333.5268\n",
            "Epoch [26/50], Loss: -333.5269\n",
            "Epoch [27/50], Loss: -333.5269\n",
            "Epoch [28/50], Loss: -333.5270\n",
            "Epoch [29/50], Loss: -333.5270\n",
            "Epoch [30/50], Loss: -333.5271\n",
            "Epoch [31/50], Loss: -333.5271\n",
            "Epoch [32/50], Loss: -333.5271\n",
            "Epoch [33/50], Loss: -333.5271\n",
            "Epoch [34/50], Loss: -333.5272\n",
            "Epoch [35/50], Loss: -333.5272\n",
            "Epoch [36/50], Loss: -333.5272\n",
            "Epoch [37/50], Loss: -333.5272\n",
            "Epoch [38/50], Loss: -333.5272\n",
            "Epoch [39/50], Loss: -333.5272\n",
            "Epoch [40/50], Loss: -333.5272\n",
            "Epoch [41/50], Loss: -333.5272\n",
            "Epoch [42/50], Loss: -333.5272\n",
            "Epoch [43/50], Loss: -333.5272\n",
            "Epoch [44/50], Loss: -333.5272\n",
            "Epoch [45/50], Loss: -333.5272\n",
            "Epoch [46/50], Loss: -333.5272\n",
            "Epoch [47/50], Loss: -333.5272\n",
            "Epoch [48/50], Loss: -333.5272\n",
            "Epoch [49/50], Loss: -333.5272\n",
            "Epoch [50/50], Loss: -333.5272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This snippet is used for generating new MNIST-like images using a trained Variational Autoencoder (VAE).\n",
        "\n",
        "1ï¸âƒ£ Set Model to Evaluation Mode\n",
        "This switches the model to evaluation mode, disabling dropout and batch normalization updates.\n",
        "\n",
        "Ensures the model behaves consistently when generating images.\n",
        "\n",
        "2ï¸âƒ£ Generate Random Latent Vectors\n",
        "torch.no_grad() â†’ Disables gradient tracking to save memory (since we donâ€™t need backpropagation for inference).\n",
        "\n",
        "torch.randn(16, latent_dim).to(device)\n",
        "\n",
        "    Generates 16 random latent vectors from a standard normal distribution (ð’©(0,1)).\n",
        "    latent_dim = 100 (from earlier) defines the dimensionality of the latent space.\n",
        "    .to(device) moves the tensor to the GPU (if available) for faster processing.\n",
        "\n",
        "3ï¸âƒ£ Decode Latent Vectors into Images\n",
        "vae.decode(z) â†’ Passes the latent vectors through the decoder part of the VAE to generate new images.\n",
        "\n",
        ".view(-1, 1, 28, 28) â†’ Reshapes the output into MNIST format (1x28x28, grayscale images).\n",
        "\n",
        ".cpu() â†’ Moves the tensor to CPU for visualization.    \n",
        "\n",
        "4ï¸âƒ£ Rescale Pixel Values to [0, 1]\n",
        "Since the model outputs images in the [-1, 1] range (due to tanh activation and normalization), this rescales them back to [0, 1] for proper visualization.\n",
        "\n",
        "5ï¸âƒ£ Plot the Generated Images\n",
        "Creates a 4Ã—4 grid (16 images total) for visualization.\n",
        "\n",
        "axs[i, j].imshow(..., cmap=\"gray\") â†’ Displays each generated image in grayscale.\n",
        ".squeeze() â†’ Removes extra dimensions ([1, 28, 28] â†’ [28, 28]).\n",
        "\n",
        ".axis(\"off\") â†’ Hides axis ticks for a clean look.\n",
        "\n"
      ],
      "metadata": {
        "id": "VvPFxPu6opsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new images\n",
        "vae.eval()\n",
        "with torch.no_grad():\n",
        "    # Sample random latent vectors\n",
        "    z = torch.randn(16, latent_dim).to(device)\n",
        "    generated = vae.decode(z).view(-1, 1, 28, 28).cpu()\n",
        "    generated = (generated + 1) / 2  # Rescale to [0, 1]\n",
        "\n",
        "    # Plot generated images\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(4, 4))\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            axs[i, j].imshow(generated[i * 4 + j].squeeze(), cmap=\"gray\")\n",
        "            axs[i, j].axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"Image generation complete!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "y_XZlfgn8S9_",
        "outputId": "7fb5fec2-87be-4c33-c9c7-9a301fa9e6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image generation complete!\n"
          ]
        }
      ]
    }
  ]
}